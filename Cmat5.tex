\subsection*{I. Généralités}
\begin{enumerate}
  \item L'ensemble $\mathcal{X}_n$ s'identifie aux fonctions de $\llbracket 1,n \rrbracket^2$ dans $\left\lbrace 0,1\right\rbrace$, il est donc fini et de cardinal 
\begin{displaymath}
  2^{(n^2)}
\end{displaymath}

  \item Exprimons le déterminant avec des permutations
\begin{displaymath}
\left|\det(M)\right|
=\left|\sum_{\sigma \in \mathfrak{S}_n}\varepsilon(\sigma) \prod_{j\in \llbracket 1,n \rrbracket}m_{\sigma(j)j}\right|
\leq \sum_{\sigma \in \mathfrak{S}_n} |\varepsilon(\sigma)| \prod_{j\in \llbracket 1,n \rrbracket}|m_{\sigma(j)j}|
\end{displaymath}
Tous les facteurs sont entre 0 et $1$ donc tous les $n!$ termes de la somme sont plus petits que $1$ d'où $|\det(M)|\leq 1$.

  \item Soient $A$ et $B$ dans $\mathcal{Y}_n$ et $\lambda$ entre $0$ et $1$. Considérons $M_\lambda = \lambda A + (1-\lambda)B$. Ses coefficients vérifient
\begin{displaymath}
\text{ terme $i,j$ de }M_\lambda = \lambda a_{i,j} + (1-\lambda)b_{i,j}\in \left[  0,1\right]   
\end{displaymath}
Donc $M_\lambda \in \mathcal{Y}_n$ ce qui prouve que $\mathcal{Y}_n$ est convexe.

  \item Comme la colonne propre $X$ est non nulle, il existe un indice $i$ tel que $0<|x_i|\geq |x_j|$ pour tous les autres $j$. On en déduit, en examinant la ligne $i$ du produit $MX$:
\begin{displaymath}
|\lambda||x_i| \leq \sum_{j=1}^n\underset{\leq 1}{\underbrace{|m_{i,j}|}}\underset{\leq |x_i|}{\underbrace{|x_j|}}  
\leq n |x_i|
\Rightarrow |\lambda| \leq n
\end{displaymath}
Si $M$ est la matrice ne contenant que des $1$ et $X$ la colonne ne contenant que des $1$, on a $MX = nX$. Il est donc possible d'obtenir des valeurs propres de module $n$.

\item
\begin{enumerate}
  \item Parmi les 16 matrices d'ordre 2 formées de 0 et de 1, on liste d'abord celles qui ne contiennent ni ligne ni colonne nulle en considérant les premières lignes possibles (il y en a 3):
\begin{displaymath}
\begin{pmatrix}
0 & 1 \\ 1 & 0  
\end{pmatrix},\;
\begin{pmatrix}
0 & 1 \\ 1 & 1  
\end{pmatrix},\;
\begin{pmatrix}
1 & 0 \\ 0 & 1  
\end{pmatrix},\;
\begin{pmatrix}
1 & 0 \\ 1 & 1  
\end{pmatrix},\;
\begin{pmatrix}
1 & 1 \\ 0 & 1  
\end{pmatrix},\;
\begin{pmatrix}
1 & 1 \\ 1 & 0  
\end{pmatrix},\;
\begin{pmatrix}
1 & 1 \\ 1 & 1  
\end{pmatrix}
\end{displaymath}
On élimine la dernière qui est de rang $1$. Il en reste donc $6$. 
\begin{multline*}
A=
\begin{pmatrix}
0 & 1 \\ 1 & 0  
\end{pmatrix},\;
B=
\begin{pmatrix}
0 & 1 \\ 1 & 1  
\end{pmatrix},\;
I=
\begin{pmatrix}
1 & 0 \\ 0 & 1  
\end{pmatrix},\;
C=
\begin{pmatrix}
1 & 0 \\ 1 & 1  
\end{pmatrix},\;
D=
\begin{pmatrix}
1 & 1 \\ 0 & 1  
\end{pmatrix},\\
E=
\begin{pmatrix}
1 & 1 \\ 1 & 0  
\end{pmatrix},
\end{multline*}
  \item Elles engendrent les matrices élémentaires car
\begin{displaymath}
E_{1,1}=E-A, \;E_{1,2}=D-I, \;E_{2,1}=C-I, \;E_{2,2}=B-A   
\end{displaymath}
Dans le cas général de l'ordre $n$. Si $i\neq j$, la matrice $I_n + E_{i,j}$ est une matrice d'opérations élémentaire donc dans $\mathcal{X}'_n$. Ceci montre que $E_{i,j}\in \Vect(\mathcal{X}'_n)$.
Pour les $E_{i,i}$, on utilise la matrice $D$ avec des $1$ sur \og mauvaise\fg~ diagonale ($d_{i,j}=\delta _{n-j+1,i}$) car $D+E_{i,i}$ est encore inversible. On engendre ainsi tous les $E_{i,i}$ sauf $E_{p,p}$ lorsque $n=2p+1$. On pourra également l'obtenir comme combinaison mais avec plus de deux matrices. La propriété reste donc vraie à l'ordre $n$. Par exemple
\begin{displaymath}
\begin{pmatrix}
 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 
\end{pmatrix} = 
D + 2I 
-\begin{pmatrix}
1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 0 & 1   
\end{pmatrix}
-\begin{pmatrix}
1 & 0 & 1 \\ 0 & 1 & 0 \\ 0 & 0 & 1   
\end{pmatrix}
\end{displaymath}

\end{enumerate}
\end{enumerate}

\subsection*{II. Maximisation du déterminant}
\begin{enumerate}
  \item Comme $\mathcal{X}_n$ est fini, l'ensemble des déterminants l'est aussi donc il admet un plus grand élément.\newline
L'ensemble $\mathcal{Y}_n$ est infini donc on ne peut affirmer que l'ensemble des déterminants soit fini. En revanche, on sait d'après I.2. qu'il est majoré. Comme toute partie de $\R$ non vide et majorée il admet une borne supérieure $y_n$.

  \item Le nombre $y_{n+1}$ est un majorant de $\left\lbrace \det(M), M\in  \mathcal{Y}_n\right\rbrace $. En effet, pour toute $M\in \mathcal{Y}_n$, on peut former une matrice $M'\in \mathcal{Y}_{n+1}$ de même déterminant en la bordant par des $0$ avec seulement un $1$ en position $1,1$. Comme $y_n$ est le plus petit des majorants, on en déduit $y_n\leq y_{n+1}$.
  
  \item Notons $X_1, \cdots, X_n$ les colonnes de la base canoniques des matrices colonnes et $C$ la colonne qui ne contient que des $1$. On peut alors écrire
\begin{displaymath}
  C_j(J) = C - X_j
\end{displaymath}
Développons le déterminant de $M$ par multilinéarité par rapport aux colonnes. Seuls contribuent les distributions où $C$ figure au plus une fois. On en déduit
\begin{multline*}
  \det(M) = (-1)^n\det(I) + (-1)^{n-1}\sum_{j=1}^n\det(X_1,\cdots,X_{j-1},C,X_{j+1},\cdots,X_n)\\
  = (-1)^{n-1}(n-1)
\end{multline*}
Ces matrices montrent que la suite des $y_n$ diverge vers $+\infty$ pour les $n$ impairs. Pour les $n$ pairs, il suffit de modifier la matrice en permutant deux lignes ou colonnes.

  \item
\begin{enumerate}
  \item On suppose $n_{i_0,j_0}\in \left] 0,1\right[$, considérons le développement du déterminant selon la colonne $j_0$. Dans ce développement, le coefficient $n_{i_0,j_0}$ intervient seulement dans le terme
\begin{displaymath}
  n_{i_0,j_0}C_{i_0,j_0}
\end{displaymath}
où $C_{i_0,j_0}$ désigne le cofacteur. Si ce cofacteur est positif ou nul, en remplaçant $n_{i_0,j_0}$ par $1$, on augmente le déterminant. Si le cofacteur est strictement négatif, en remplaçant cette fois $n_{i_0,j_0}$ par $0$ on l'augmente aussi. On peut donc former une matrice $N'$ comme le demande l'énoncé.

  \item Pour une matrice $M$ quelconque dans $\mathcal{Y}_n$, en procédant systématiquement comme dans la question précédente, on peut remplacer tous les coefficients dans l'intervalle ouvert par des $0$ ou des $1$ et obtenir finalement une matrice $M'\in \mathcal{X}_n$ telle que $\det(M)\leq \det(M')$. On en déduit que $x_n$ est un majorant de $y_n$ donc que $x_n=y_n$. 
\end{enumerate}
\end{enumerate}

\subsection*{III. Matrices de permutations}
\begin{enumerate}
  \item Voir exercice sur les matrices de permutation. On trouve
\begin{displaymath}
  P_\sigma P_{\sigma'} = P_{\sigma \circ \sigma'} \hspace{0.5cm} \trans P_\sigma = (P_\sigma)^{-1}= P_{\sigma^{-1}}
\end{displaymath}

  \item On utilise la définition de $\mathcal{O}_n$ et les propriétés du déterminant.
\begin{displaymath}
\trans A \, A = I_n \Rightarrow \det(\trans A \, A ) = \det(I_n)\Rightarrow (\det(A))^2 = 1  
\end{displaymath}

  \item Une matrice de permutation ne contient que des $0$ et des $1$ donc elle est dans $\mathcal{X}_n$. Elle est aussi orthogonale car 
\begin{displaymath}
  P_{\sigma} \trans P_{\sigma} = P_{\sigma} P_{\sigma^{-1}}= P_{\Id} = I_n
\end{displaymath}
Réciproquement, soit $M$ une matrice orthogonale ne contenant que des $0$ et des $1$. Comme elle est othogonale, chaque ligne $i_0$ contient au moins un $1$. Soit $j_0$ tel que $m_{i_0,j_0}=1$.\newline
Pour tous les $j\neq j_0$, le terme $j, j_0$ de $\trans M M$ est nul :
\begin{displaymath}
\forall j\neq j_0,\;  \sum _{k=1}^n m_{k,j}m_{k,j_0} = 0 \Rightarrow m_{i_0,j} = 0
\end{displaymath}
si les $1$ \og se trouvaient\fg~ la somme ne pourrait pas être nulle. On en déduit que $m_{i_0,j_0}$ est le seul terme non nul de la ligne $i_0$.\newline
Cela permet de définir une fonction $\varphi$ de $\llbracket 1,n \rrbracket$ dans lui même telle que $m_{i,\varphi(i)}$ soit le seul terme égal à $1$ de la ligne $i$. Si cette application n'était pas injective, une même ligne se trouverait deux fois ce qui est en contradiction avec le fait que la matrice est inversible. On en déduit que chaque colonne contient un seul $1$ et que $M=P_{\varphi^{-1}}$.

  \item 
\begin{enumerate}
  \item   Introduisons la colonne
\begin{displaymath}
  X_{\omega}=
\begin{pmatrix}
  1 \\ \omega \\ \omega^2 \\ \vdots \\ \omega^{n-1}
\end{pmatrix}
\end{displaymath}
On vérifie facilement que
\begin{displaymath}
P_{c}X_\omega = \omega X_\omega  
\end{displaymath}
Ce qui montre que $\omega$ est une valeur propre complexe de $P_c$ de vecteur propre $X_\omega$. De même, pour tout $u\in \U_n$,
\begin{displaymath}
  X_{u}=
\begin{pmatrix}
  1 \\ u \\ u^2 \\ \vdots \\ u^{n-1}
\end{pmatrix}
\Rightarrow P_{c}X_u = u X_u  
\end{displaymath}

  \item Une permutation quelconque se décompose en cycles disjoints. Pour chaque cycle (disons de longueur $r$), on peut trouver $r$ vecteurs propres formés comme au dessus avec des éléments de $\U_r$ (chacun étant attaché à une valeur propre élément de $\U_r$) mais seulement pour les indices du support du cycle et des $0$ pour les autres.
\end{enumerate}
  
  \item On se donne une matrice $M$ à coefficients dans $\N$ telle que l'ensemble des coefficients de toutes les $M^k$ avec $k\in \N^*$ soit fini. On en déduit que l'ensemble $\left\lbrace M^k, k\in \N\right\rbrace$ est fini. L'application $k\mapsto M^k$ ne peut pas être injective car $\N^*$ est infini. Il existe donc $k<k'$ tels que
\begin{displaymath}
M^k = M^{k'} \Rightarrow M^{k'-k} = I_n  
\end{displaymath}
On en déduit que $M$ est inversible et que son inverse $M'$ est une puissance de $M$ donc il est aussi à coefficients dans $\N$. On peut alors raisonner comme pour les matrices orthogonales à partir de $M'M$.\newline
Pour tout $j_0$, il existe $i_0$ tel que $m'_{i_0,j_0}\neq 0$ car $M'$ est inversible. Pour tout $i\neq i_0$, le terme $i_0,i$ de $M'M$ est nul d'où:
\begin{displaymath}
  \sum_{k=1}^n m'_{i_0,k}m_{k,i} = 0 \;\text{ et } m'_{i_0,j_0}\neq 0 \Rightarrow m_{j_0,i}=0
\end{displaymath}
Autrement dit la ligne $j_0$ de $M$ contient au plus un coefficient égal à $1$. Comme elle doit en contenir un pour être inversible elle en contient un seul et c'est une matrice de permutation. 
\end{enumerate}


\subsection*{IV. Matrices aléatoires}
\subsubsection*{A. Génération par une colonne aléatoire}
\begin{enumerate}
  \item Lorsque toutes les lois prennent la même valeur, celle ci peut être $0$ ou $1$ les deux événements sont incompatibles et les variables sont indépendantes donc
\begin{displaymath}
  \p\left( X_1=X_2=\cdots =X_n\right) = p^n + (1-p)^n
\end{displaymath}

  \item La variable $S$ suit une loi binomiale de paramètres $n$ et $p$. Voir cours sur une variable binomiale comme somme de variables de Bernoulli indépendantes.
  
  \item Pour tous les couples $(i,j)$, les variables $X_{i,j}=X_i\,X_j$ ne prennent que les valeurs $0$ et $1$. Comme elles sont mutuellement indépendantes, si $i\neq j$, la variable $X_{i,j}$ suit une loi de Bernoulli de paramètre $p^2$. En revanche $X_{i,i} = X_i^2 = X_i$ suit une loi de Bernoulli de paramètre $p$.
  \item 
\begin{enumerate}
  \item Le terme $i,j$ de $M(\omega)$ est $X_i(\omega)X_j(\omega)=X_{i,j}(\omega)\in\left\lbrace 0,1 \right\rbrace$. Donc $M(\omega)$ appartient à $\mathcal{X}_n$. La variable aléatoire $M$ est à valeurs dans $\mathcal{X}_n$.

  \item La diagonale de $M$ contient $n$ coefficients tous dans $\left\lbrace 0,1\right\rbrace$ donc $\tr(M)\in \zeron$. Les colonnes de $M(\omega)$ sont $X_1(\omega)U(\omega), \cdots, X_n(\omega)U(\omega)$. Elle sont toutes dans $\Vect(U(\omega))$ donc le rang est inférieur ou égal à $1$.
  
  \item On utilise l'associativité du produit matriciel et le fait qu'un produit ligne$\times$colonne est un élément du corps donc peut se factoriser. De plus, ici 
\begin{displaymath}
  \trans U(\omega) \,U(\omega) = \sum_{k=1}^n X_k(\omega)^2 = \sum_{k=1}^n X_k(\omega) = S(\omega)
\hspace{0.5cm}
\end{displaymath}
En transposant l'expression de $M$, on s'aperçoit que $M$ est symétrique et
\begin{displaymath}
  M^2 = U\left( \trans U\, U\right) \trans U = S\, U\,\trans U = S M 
\end{displaymath}
On en déduit la caractrisation proposée.
\end{enumerate}
  
  \item D'après l'expression de $M$, $\tr(M) = S$ qui est suit une loi binomiale de paramètres $n$ et $p$ donc son espérance est $np$ et sa variance $np(1-p)$.\newline
  Comme le rang est $0$ ou $1$, il s'agit d'une variable de Bernoulli. Son paramètre est la probabilité que la colonne $U$ ne soit pas nulle c'est à dire $1-(1-p)^n$ puisque la probabilité de l'événement contraire (la colonne est nulle) est $(1-p)^n$.
  
  \item On a déjà vu que $M^2 = SM$. On en déduit par récurrence que 
\begin{displaymath}
\forall k\geq 2, \; M^k = S^{k-1} M  
\end{displaymath}
Pour une issue $\omega$, $S(\omega)\in \zeron$ donc la suite géométrique $\left( S(\omega)^k\right)_{k\in \N}$ est convergente si et seulement si $S(\omega)\in\left\lbrace 0,1\right\rbrace$. La probabilité de cet événement est 
\begin{displaymath}
(1-p)^n + np(1-p)^{n-1} = (1-p)^{n-1}(1+(n-1)p)  
\end{displaymath} 
\end{enumerate}

\subsubsection*{B. Génération par remplissage aléatoire}
\begin{enumerate}
  \item La matrice départ ne contient que des $0$ donc $N_1$ est une somme de $m=n^2$ variables de Bernoulli de paramètre $p$. On en déduit que $N_1$ suit une loi binomiale de paramètres $m$ et $p$.\newline
Pour la deuxième vague, si $i$ coefficients ont pris la valeur $1$, il en reste $m-i$ modifiables donc la loi conditionnelle de $N_2$ sachant $N_1=i$ est binomiale de paramètres $m-i$, $p$.\newline
Les variables $N_1$ et $N_2$ ne sont pas indépendantes. En effet, 
\begin{displaymath}
\p( (N_1=m) \cap (N_2=1))=0  \neq \p((N_1=3))\,\p((N_2=1))
\end{displaymath}
car si toutes les valeurs ont été passées à $1$ lors de la première vague, aucune ne peut l'être lors de la seconde alors que $\p((N_1=3))$ et $\p((N_2=1))$ sont non nulles.
  \item Comme les passages de $0$ à $1$ pour les coefficients sont indépendants,
\begin{displaymath}
  \p(T_{i,j}=k) = q^{k-1}p
\end{displaymath}
Le coefficient n'a pas été changé lors des $k-1$ premiers passage puis il l'a été au $k$-ème.
  \item D'après la question précédente
\begin{displaymath}
\p(k\leq T_{i,j} \leq s) = q^{k-1}p + q^{k}p + \cdots + q^{s-1}p = q^{k-1}p\left(1+ q + \cdots + q ^{s-k} \right)    
\end{displaymath}
Pour $s$ en $+\infty$, elle converge vers
\begin{displaymath}
  \p(T_{i,j}\geq k) = q^{k-1}p\frac{1}{1-q} = q^{k-1}
\end{displaymath}

  \item La variable $S_r$ représente le nombre de $1$ dans la matrice aléatoire après $r$ passages. Elle prend ses valeurs dans $\llbracket 0,m \rrbracket$. Pour un $k \in \llbracket 0,m \rrbracket$, évaluons $\p(S_r = l)$. Classons les issues suivant l'ensemble des $l$ places de la matrice contenant un $1$ à la fin. On obtient ainsi $\binom{m}{l}$ événements qui ont la même probabilité.\newline
  Quelle est cette probabilité? Fixons un ensemble d'indices $\mathcal{I}\subset \llbracket 1,n\rrbracket^2$ à $l$ éléments. On cherche la probabilité de l'événement
\begin{displaymath}
  \left( \bigcap_{(i,j)\in \mathcal{I}}\left( T_{i,j}\leq r\right)\right)  \cap \left( \bigcap_{(i,j) \notin \mathcal{I}}\left( T_{i,j}> r\right)\right) 
\end{displaymath}
Par hypothèse, les $T_{i,j}$ sont indépendants et suivent la même loi d'où
\begin{displaymath}
\p(S_r = l) = \binom{m}{l}\left(\underset{\text{avec }p=1-q}{\underbrace{p+qp+\cdots+q^{r-1}p}} \right)^l  \left( q^{r}\right)^{m-l} 
= \binom{m}{l}\left( 1-q^r\right)^l q^{r(m-l)}
\end{displaymath}
\end{enumerate}
